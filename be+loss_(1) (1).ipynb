{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGSVuK1Rb2pT",
        "outputId": "5f170832-8d3a-49b7-fe90-5f6bf90213ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "RcpSk2WTcHJA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXK7Jgq_dFhJ",
        "outputId": "0832f1e3-930d-4000-ed24-44224a1e8d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸ” Found: /content/drive/MyDrive/DATASET/masked_images.zip. Extracting to /content/drive/MyDrive/DATASET/masked_images...\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "dataset_path = \"/content/drive/MyDrive/DATASET\"\n",
        "masked_images_zip = os.path.join(dataset_path, \"masked_images.zip\")\n",
        "\n",
        "# Target extraction path\n",
        "masked_images_path = os.path.join(dataset_path, \"masked_images\")\n",
        "\n",
        "# Ensure target directory exists\n",
        "os.makedirs(masked_images_path, exist_ok=True)\n",
        "\n",
        "# Function to extract the masked images zip file\n",
        "def extract_masked_images(zip_path, extract_to):\n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"ðŸ” Found: {zip_path}. Extracting to {extract_to}...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "                print(f\"âœ… Successfully extracted {zip_path} to {extract_to}\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"âŒ ERROR: {zip_path} is corrupted or not a valid ZIP file.\")\n",
        "    else:\n",
        "        print(f\"âŒ ERROR: {zip_path} not found!\")\n",
        "\n",
        "# Extract only masked images\n",
        "extract_masked_images(masked_images_zip, masked_images_path)\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists(masked_images_path):\n",
        "    print(\"ðŸ“‚ Masked Images:\", os.listdir(masked_images_path)[:5])  # Show first 5 files\n",
        "else:\n",
        "    print(\"âŒ ERROR: Masked images folder not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtDwZnSpdXVX"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "dataset_path = \"/content/drive/MyDrive/DATASET\"\n",
        "resized_images_zip = os.path.join(dataset_path, \"resized images.zip\")\n",
        "\n",
        "# Target extraction path\n",
        "resized_images_path = os.path.join(dataset_path, \"resized_images\")\n",
        "\n",
        "# Ensure target directory exists\n",
        "os.makedirs(resized_images_path, exist_ok=True)\n",
        "\n",
        "# Function to extract resized images zip file\n",
        "def extract_resized_images(zip_path, extract_to):\n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"ðŸ” Found: {zip_path}. Extracting to {extract_to}...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "                print(f\"âœ… Successfully extracted {zip_path} to {extract_to}\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"âŒ ERROR: {zip_path} is corrupted or not a valid ZIP file.\")\n",
        "    else:\n",
        "        print(f\"âŒ ERROR: {zip_path} not found!\")\n",
        "\n",
        "# Extract only resized images\n",
        "extract_resized_images(resized_images_zip, resized_images_path)\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists(resized_images_path):\n",
        "    print(\"ðŸ“‚ Resized Images:\", os.listdir(resized_images_path)[:5])  # Show first 5 files\n",
        "else:\n",
        "    print(\"âŒ ERROR: Resized images folder not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgBDggmsdlCG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the specific path for masked images\n",
        "masked_images_data_path = \"/content/drive/MyDrive/DATASET/masked_images/masked_images/masked_images\"\n",
        "\n",
        "# Function to count files in a directory\n",
        "def count_files_in_directory(directory_path):\n",
        "    if os.path.exists(directory_path):\n",
        "        files = os.listdir(directory_path)  # List all files and directories\n",
        "        file_count = len([f for f in files if os.path.isfile(os.path.join(directory_path, f))])  # Count only files\n",
        "        print(f\"ðŸ“‚ {directory_path} contains {file_count} files.\")\n",
        "    else:\n",
        "        print(f\"âŒ ERROR: Directory {directory_path} does not exist!\")\n",
        "\n",
        "# Count files in the specific masked images data directory\n",
        "count_files_in_directory(masked_images_data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg_mTQ7McHMG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Paths in Google Drive\n",
        "base_dir = \"/content/drive/MyDrive/DATASET\"  # Updated to your 'DATASET' folder\n",
        "images_dir = os.path.join(base_dir, \"resized_images/resized images\")  # Path for resized images\n",
        "masks_dir = os.path.join(base_dir, \"masked_images/masked_images/masked_images\")  # Path for masked images\n",
        "output_dir = \"/content/drive/MyDrive/DATASET/data_split2\"  # Location for split dataset\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Verify output directory exists\n",
        "if os.path.exists(output_dir):\n",
        "    print(f\"âœ… Output directory exists at: {output_dir}\")\n",
        "else:\n",
        "    print(f\"âŒ Output directory not found at: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA-T6s0jrPEU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkjC-FTUcHOC"
      },
      "outputs": [],
      "source": [
        "for folder in [\"train\", \"val\", \"test\"]:\n",
        "    os.makedirs(os.path.join(output_dir, folder, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, folder, \"masks\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaclqCZ1cHQr"
      },
      "outputs": [],
      "source": [
        "image_filenames = sorted(os.listdir(images_dir))\n",
        "mask_filenames = sorted(os.listdir(masks_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Rhu112cHTV"
      },
      "outputs": [],
      "source": [
        "assert image_filenames == mask_filenames, \"Image and mask filenames do not match!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VY2dY3mcHV0"
      },
      "outputs": [],
      "source": [
        "train_images, temp_images, train_masks, temp_masks = train_test_split(\n",
        "    image_filenames, mask_filenames, test_size=0.3, random_state=42\n",
        ")\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(\n",
        "    temp_images, temp_masks, test_size=0.5, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doXp3RZQcHZQ"
      },
      "outputs": [],
      "source": [
        "def copy_files(file_list, source_dir, dest_dir):\n",
        "    for file in file_list:\n",
        "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUS_crVxfQhQ"
      },
      "outputs": [],
      "source": [
        "copy_files(train_images, images_dir, os.path.join(output_dir, \"train/images\"))\n",
        "copy_files(train_masks, masks_dir, os.path.join(output_dir, \"train/masks\"))\n",
        "copy_files(val_images, images_dir, os.path.join(output_dir, \"val/images\"))\n",
        "copy_files(val_masks, masks_dir, os.path.join(output_dir, \"val/masks\"))\n",
        "copy_files(test_images, images_dir, os.path.join(output_dir, \"test/images\"))\n",
        "copy_files(test_masks, masks_dir, os.path.join(output_dir, \"test/masks\"))\n",
        "\n",
        "print(\"Dataset successfully split into train, val, and test sets!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfkBV0lEgzhi"
      },
      "source": [
        "DATA LOADER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_0gIs-gfQeI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acaXlDlafQb8"
      },
      "outputs": [],
      "source": [
        "class LaneDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_filenames = sorted(os.listdir(images_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(masks_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # Grayscale mask\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpXOcM_e4b45"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezNxVdoTwhSE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class LaneDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_filenames = sorted(os.listdir(images_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(masks_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # Grayscale mask\n",
        "\n",
        "        # Define separate transformations for images and masks\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = transforms.ToTensor()(mask)  # Convert mask to tensor separately\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzTBkB2UwhO3"
      },
      "outputs": [],
      "source": [
        "# Define transforms for the images\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize to desired shape\n",
        "    transforms.ToTensor(),           # Convert image to tensor\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = LaneDataset(\n",
        "    images_dir=\"/content/drive/MyDrive/DATASET/data_split2/train/images\",\n",
        "    masks_dir=\"/content/drive/MyDrive/DATASET/data_split2/train/masks\",\n",
        "    transform=image_transform\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Check one batch\n",
        "for images, masks in train_loader:\n",
        "    print(\"Image batch shape:\", images.shape)\n",
        "    print(\"Mask batch shape:\", masks.shape)\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAK4-EKwwhMZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e10nCVmg1S8"
      },
      "outputs": [],
      "source": [
        "train_images_dir = \"/content/drive/MyDrive/DATASET/data_split2/train/images\"\n",
        "train_masks_dir = \"/content/drive/MyDrive/DATASET/data_split2/train/masks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8YZcTLJg1Px"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Ensure images are resized\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTpUlhM3r05n"
      },
      "outputs": [],
      "source": [
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize image\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()  # No normalization for masks\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaNWtDQ6g1ML"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = LaneDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uWx_-N5hCmi"
      },
      "outputs": [],
      "source": [
        "for images, masks in train_loader:\n",
        "    print(\"Batch of images:\", images.shape)\n",
        "    print(\"Batch of masks:\", masks.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU97cgkKw4RC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM7HMor1r4cQ"
      },
      "outputs": [],
      "source": [
        "def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "    mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    mask = Image.open(mask_path).convert(\"L\")  # Grayscale mask\n",
        "\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.mask_transform:\n",
        "        mask = self.mask_transform(mask)\n",
        "\n",
        "    return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M30mFjF6sATi"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "for images, masks in train_loader:\n",
        "    print(\"âœ… Batch of images shape:\", images.shape)  # (batch_size, 3, 224, 224)\n",
        "    print(\"âœ… Batch of masks shape:\", masks.shape)    # (batch_size, 1, 224, 224)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPyM8cFkhHvy"
      },
      "source": [
        "MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oKMvX_FhCjU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe8AwKq7hGT3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Contracting Path\n",
        "        self.enc1 = self.double_conv(in_channels, 64)\n",
        "        self.enc2 = self.double_conv(64, 128)\n",
        "        self.enc3 = self.double_conv(128, 256, dropout=True)\n",
        "        self.enc4 = self.double_conv(256, 512, dropout=True)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.double_conv(512, 1024, dropout=True)\n",
        "\n",
        "        # Expanding Path\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = self.double_conv(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = self.double_conv(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.double_conv(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.double_conv(128, 64)\n",
        "\n",
        "        # Output Layer\n",
        "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels, dropout=False):\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(0.3))  # Dropout to prevent overfitting\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Contracting Path\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        # Expanding Path\n",
        "        up4 = self.upconv4(bottleneck)\n",
        "        dec4 = self.dec4(torch.cat([up4, enc4], dim=1))\n",
        "\n",
        "        up3 = self.upconv3(dec4)\n",
        "        dec3 = self.dec3(torch.cat([up3, enc3], dim=1))\n",
        "\n",
        "        up2 = self.upconv2(dec3)\n",
        "        dec2 = self.dec2(torch.cat([up2, enc2], dim=1))\n",
        "\n",
        "        up1 = self.upconv1(dec2)\n",
        "        dec1 = self.dec1(torch.cat([up1, enc1], dim=1))\n",
        "\n",
        "        return torch.sigmoid(self.out(dec1))  # Sigmoid for binary segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYA6p1qWhNqd"
      },
      "source": [
        "RUN EVERYTIME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9_YLnedhGQR"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3neP0YQ2hMDO"
      },
      "outputs": [],
      "source": [
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPvEhmG2hL_k"
      },
      "outputs": [],
      "source": [
        "scaler = torch.amp.GradScaler()\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6yvlgLHxoaN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Ensure correct GradScaler initialization\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        masks = masks.float()  # Ensure masks are float for BCEWithLogitsLoss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():  # Mixed precision training\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backpropagation with scaler\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JoIp8qWhSVF"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        if scaler:\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfJnGqr5pAyh"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = LaneDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRoJxSW7hSRy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Switch model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to compute Intersection over Union (IoU)\n",
        "def compute_iou(pred, target, threshold=0.5):\n",
        "    pred = (pred > threshold).float()\n",
        "    intersection = (pred * target).sum()\n",
        "    union = (pred + target).sum() - intersection\n",
        "    iou = intersection / union\n",
        "    return iou.item()\n",
        "\n",
        "# Evaluate on test dataset\n",
        "test_images_dir = \"/content/drive/MyDrive/DATASET/data_split2/test/images\"\n",
        "test_masks_dir = \"/content/drive/MyDrive/DATASET/data_split2/test/masks\"\n",
        "test_dataset = LaneDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "iou_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        outputs = torch.sigmoid(outputs)  # Apply sigmoid since we used BCEWithLogitsLoss\n",
        "        iou = compute_iou(outputs, masks)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "print(f\"Average IoU on Test Set: {np.mean(iou_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfdT1fe44BBe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_TxHqZYhYRr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFyLG8ephZAC"
      },
      "outputs": [],
      "source": [
        "class LaneDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_filenames = sorted(os.listdir(images_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(masks_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CknOQwTFx7gj"
      },
      "outputs": [],
      "source": [
        "class LaneDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, image_transform=None, mask_transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_filenames = sorted(os.listdir(images_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(masks_dir))\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
        "\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB0CbfRThY9-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Contracting Path\n",
        "        self.enc1 = self.double_conv(in_channels, 64)\n",
        "        self.enc2 = self.double_conv(64, 128)\n",
        "        self.enc3 = self.double_conv(128, 256)\n",
        "        self.enc4 = self.double_conv(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.double_conv(512, 1024)\n",
        "\n",
        "        # Expanding Path\n",
        "        self.upconv4 = self.upconv(1024, 512)\n",
        "        self.dec4 = self.double_conv(1024, 512)\n",
        "\n",
        "        self.upconv3 = self.upconv(512, 256)\n",
        "        self.dec3 = self.double_conv(512, 256)\n",
        "\n",
        "        self.upconv2 = self.upconv(256, 128)\n",
        "        self.dec2 = self.double_conv(256, 128)\n",
        "\n",
        "        self.upconv1 = self.upconv(128, 64)\n",
        "        self.dec1 = self.double_conv(128, 64)\n",
        "\n",
        "        # Output Layer\n",
        "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv(self, in_channels, out_channels):\n",
        "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Contracting Path\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(F.max_pool2d(enc1, kernel_size=2))\n",
        "        enc3 = self.enc3(F.max_pool2d(enc2, kernel_size=2))\n",
        "        enc4 = self.enc4(F.max_pool2d(enc3, kernel_size=2))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(F.max_pool2d(enc4, kernel_size=2))\n",
        "\n",
        "        # Expanding Path\n",
        "        up4 = self.upconv4(bottleneck)\n",
        "        dec4 = self.dec4(torch.cat([up4, enc4], dim=1))\n",
        "\n",
        "        up3 = self.upconv3(dec4)\n",
        "        dec3 = self.dec3(torch.cat([up3, enc3], dim=1))\n",
        "\n",
        "        up2 = self.upconv2(dec3)\n",
        "        dec2 = self.dec2(torch.cat([up2, enc2], dim=1))\n",
        "\n",
        "        up1 = self.upconv1(dec2)\n",
        "        dec1 = self.dec1(torch.cat([up1, enc1], dim=1))\n",
        "\n",
        "        # Output Layer (logits)\n",
        "        return self.out(dec1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMhcsQpKvI5B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class LaneDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_filenames = sorted(os.listdir(images_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(masks_dir))\n",
        "        self.transform = transform  # Store transform function\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "\n",
        "        image = cv2.imread(img_path)  # Read image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read mask (grayscale)\n",
        "\n",
        "        # Normalize mask (convert to binary if needed)\n",
        "        mask = (mask > 128).astype(np.float32)\n",
        "\n",
        "        # Convert NumPy arrays to PIL Image\n",
        "        image = transforms.ToPILImage()(image)\n",
        "        mask = transforms.ToPILImage()(mask)\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYJSyFIYvLBF"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match UNet input\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "])\n",
        "\n",
        "# Paths\n",
        "train_images_dir = \"/content/drive/MyDrive/DATASET/data_split2/train/images\"\n",
        "train_masks_dir = \"/content/drive/MyDrive/DATASET/data_split2/train/masks\"\n",
        "test_images_dir = \"/content/drive/MyDrive/DATASET/data_split2/test/images\"\n",
        "test_masks_dir = \"/content/drive/MyDrive/DATASET/data_split2/test/masks\"\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = LaneDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "test_dataset = LaneDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T38qZ_rEvM16"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet().to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Best for binary segmentation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n",
        "epochs = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE1BDbOCtYZN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWR8UV388UC0"
      },
      "outputs": [],
      "source": [
        "# Dice Loss Function\n",
        "def dice_loss(pred, target, smooth=1e-6):\n",
        "    pred = torch.sigmoid(pred)  # Apply sigmoid since BCEWithLogitsLoss is used\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum()\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "    return 1 - dice  # Dice loss is 1 - Dice score\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            bce_loss = criterion(outputs, masks)\n",
        "            dice_loss_val = dice_loss(outputs, masks)\n",
        "            total_loss = bce_loss + dice_loss_val\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(total_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzGBVJKq1zbO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Intersection over Union (IoU) function\n",
        "def compute_iou(pred, target, threshold=0.5):\n",
        "    pred = torch.sigmoid(pred)  # Apply sigmoid to convert logits to probability\n",
        "    pred = (pred > threshold).float()\n",
        "    intersection = (pred * target).sum()\n",
        "    union = (pred + target).sum() - intersection\n",
        "    return (intersection / union).item()\n",
        "\n",
        "# Evaluate on Test Set\n",
        "model.eval()\n",
        "iou_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        iou = compute_iou(outputs, masks)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "print(f\"Average IoU on Test Set: {np.mean(iou_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHFIy8zL4mds"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize Predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (images, masks) in enumerate(test_loader):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        outputs = torch.sigmoid(outputs)  # Convert to probability\n",
        "\n",
        "        # Plot images\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        axes[0].imshow(images[0].cpu().numpy().transpose(1, 2, 0))\n",
        "        axes[0].set_title(\"Input Image\")\n",
        "        axes[1].imshow(masks[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "        axes[1].set_title(\"Ground Truth Mask\")\n",
        "        axes[2].imshow(outputs[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "        axes[2].set_title(\"Predicted Mask\")\n",
        "        plt.show()\n",
        "\n",
        "        if i == 2:  # Show only first 3 images\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npgO-AgiWFfv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the save path inside Google Drive\n",
        "save_path = \"/content/drive/MyDrive/DATASET/lane_detection_model.pth\"\n",
        "\n",
        "# Save the entire model\n",
        "torch.save(model, save_path)\n",
        "\n",
        "# Save only model weights (recommended)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/DATASET/lane_detection_model_state.pth\")\n",
        "\n",
        "print(f\"Model saved at: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSvnnMiJWJEO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load entire model\n",
        "model = torch.load(\"/content/drive/MyDrive/DATASET/lane_detection_model.pth\")\n",
        "model.eval()\n",
        "\n",
        "# Load only state_dict (recommended)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/DATASET/lane_detection_model_state.pth\"))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H0vbe9HBnjP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def process_video(input_video, output_video=\"output_video.mp4\"):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # Stop if video ends\n",
        "\n",
        "        # Preprocess frame\n",
        "        frame_resized = cv2.resize(frame, (224, 224))\n",
        "        input_tensor = transform(Image.fromarray(frame_resized)).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = model(input_tensor)\n",
        "            mask = (torch.sigmoid(mask).squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_LINEAR) * 255\n",
        "\n",
        "        # Overlay lane mask on original frame\n",
        "        overlay = frame.copy()\n",
        "        overlay[mask > 0] = [0, 255, 0]  # Green overlay for detected lane\n",
        "        combined = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
        "\n",
        "        out.write(combined)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved as: {output_video}\")\n",
        "\n",
        "# Example usage\n",
        "process_video(\"/content/straight input2.mp4\", \"lane_detection_output.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoW9a-2hkbCr"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"lane_detection_full_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm_5gwOPmBEu"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"lane_detection_full_model.pth\", map_location=device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W79fpq2o80TT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class LaneTracker:\n",
        "    def __init__(self, process_variance=0.1, measurement_variance=0.1, ema_alpha=0.6):\n",
        "        self.kalman = cv2.KalmanFilter(4, 2)  # 4 state (x, y, dx, dy), 2 measurement (x, y)\n",
        "        self.kalman.transitionMatrix = np.array([[1, 0, 1, 0],\n",
        "                                                 [0, 1, 0, 1],\n",
        "                                                 [0, 0, 1, 0],\n",
        "                                                 [0, 0, 0, 1]], dtype=np.float32)\n",
        "        self.kalman.measurementMatrix = np.array([[1, 0, 0, 0],\n",
        "                                                  [0, 1, 0, 0]], dtype=np.float32)\n",
        "        self.kalman.processNoiseCov = np.eye(4, dtype=np.float32) * process_variance\n",
        "        self.kalman.measurementNoiseCov = np.eye(2, dtype=np.float32) * measurement_variance\n",
        "        self.ema_alpha = ema_alpha  # Smoothing factor for EMA\n",
        "        self.prev_mask = None\n",
        "\n",
        "    def update(self, detected_pts):\n",
        "        if detected_pts is not None:\n",
        "            self.kalman.correct(np.array(detected_pts, dtype=np.float32))\n",
        "        return self.kalman.predict()[:2].flatten()\n",
        "\n",
        "    def smooth_mask(self, mask):\n",
        "        if self.prev_mask is None:\n",
        "            self.prev_mask = mask.astype(np.float32)\n",
        "        else:\n",
        "            self.prev_mask = (self.ema_alpha * mask + (1 - self.ema_alpha) * self.prev_mask).astype(np.uint8)\n",
        "        return self.prev_mask\n",
        "\n",
        "def process_video(input_video, output_video=\"output_video.mp4\"):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    tracker = LaneTracker()\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Preprocess frame\n",
        "        frame_resized = cv2.resize(frame, (224, 224))\n",
        "        input_tensor = transform(Image.fromarray(frame_resized)).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = model(input_tensor)\n",
        "            mask = (torch.sigmoid(mask).squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_LINEAR) * 255\n",
        "\n",
        "        # Morphological processing for better continuity\n",
        "        kernel = np.ones((5, 5), np.uint8)\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        # Apply EMA smoothing\n",
        "        mask = tracker.smooth_mask(mask)\n",
        "\n",
        "        # Find lane positions\n",
        "        lane_pts = cv2.findNonZero(mask)\n",
        "        if lane_pts is not None:\n",
        "            avg_lane_pos = np.mean(lane_pts, axis=0).flatten()\n",
        "            tracked_pos = tracker.update(avg_lane_pos)\n",
        "\n",
        "        # Overlay lane mask on original frame\n",
        "        overlay = frame.copy()\n",
        "        overlay[mask > 0] = [0, 255, 0]  # Green overlay for detected lane\n",
        "        combined = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
        "\n",
        "        out.write(combined)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved as: {output_video}\")\n",
        "\n",
        "# Example usage\n",
        "process_video(\"/content/straight input2.mp4\", \"lane_detection_output_kalman_ema.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adCOvA029o7-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load(\"/content/lane_detection_full_model.pth\", map_location=device)\n",
        "model.eval()\n",
        "\n",
        "# Kalman Filter Setup\n",
        "kalman = cv2.KalmanFilter(4, 2)  # 4 state variables, 2 measurements\n",
        "kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
        "kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
        "kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03\n",
        "\n",
        "# EMA Parameters\n",
        "alpha = 0.6  # Adjust for smoother offset tracking\n",
        "ema_offset = 0  # Initial EMA offset\n",
        "\n",
        "def process_video(input_video, output_video=\"output_video.mp4\"):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    prev_hybrid_offset = None\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_resized = cv2.resize(frame, (224, 224))\n",
        "        input_tensor = transform(Image.fromarray(frame_resized)).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = model(input_tensor)\n",
        "            mask = (torch.sigmoid(mask).squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_LINEAR) * 255\n",
        "\n",
        "        # Calculate lane offset\n",
        "        lane_center = np.mean(np.where(mask > 0)[1]) if np.any(mask > 0) else width // 2\n",
        "        frame_center = width // 2\n",
        "        measured_offset = lane_center - frame_center\n",
        "\n",
        "        # Apply Kalman and EMA Filtering\n",
        "        prediction = kalman.predict()\n",
        "        estimated_offset = prediction[0][0]\n",
        "        measurement = np.array([[np.float32(measured_offset)], [0]], np.float32)\n",
        "        kalman.correct(measurement)\n",
        "        global ema_offset\n",
        "        ema_offset = alpha * measured_offset + (1 - alpha) * ema_offset\n",
        "        hybrid_offset = (estimated_offset + ema_offset) / 2\n",
        "\n",
        "        prev_hybrid_offset = hybrid_offset\n",
        "\n",
        "        # Overlay lane mask\n",
        "        overlay = frame.copy()\n",
        "        overlay[mask > 0] = [0, 255, 0]\n",
        "        combined = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
        "\n",
        "        # Display offsets\n",
        "        cv2.putText(combined, f\"Hybrid Offset: {int(hybrid_offset)} px\", (10, 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "        out.write(combined)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved as: {output_video}\")\n",
        "\n",
        "# Example usage\n",
        "process_video(\"/content/straight input2.mp4\", \"lane_detection_output.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load(\"/content/lane_detection_full_model.pth\", map_location=device)\n",
        "model.eval()\n",
        "\n",
        "# Kalman Filter Setup (More Stable)\n",
        "kalman = cv2.KalmanFilter(4, 2)\n",
        "kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
        "kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
        "kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.01  # Lower noise for smoother tracking\n",
        "\n",
        "# EMA Parameters\n",
        "alpha = 0.8  # Higher smoothing\n",
        "ema_offset = 0\n",
        "\n",
        "def process_video(input_video, output_video=\"output_video.mp4\"):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),  # Keeps aspect ratio\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])  # Normalize input\n",
        "    ])\n",
        "\n",
        "    prev_lane_center = None\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_resized = cv2.resize(frame, (256, 256))\n",
        "        input_tensor = transform(Image.fromarray(frame_resized)).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = model(input_tensor)\n",
        "            mask = (torch.sigmoid(mask).squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_LINEAR) * 255\n",
        "\n",
        "        # **Morphological filtering to stabilize lanes**\n",
        "        kernel = np.ones((5, 5), np.uint8)\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "\n",
        "        # Calculate lane position with previous frame stability\n",
        "        lane_pixels = np.where(mask > 0)\n",
        "        if lane_pixels[0].size > 0:\n",
        "            lane_center = np.mean(lane_pixels[1])  # X-axis mean\n",
        "            if prev_lane_center is not None:\n",
        "                lane_center = 0.8 * prev_lane_center + 0.2 * lane_center  # Smooth transition\n",
        "        else:\n",
        "            lane_center = prev_lane_center if prev_lane_center is not None else width // 2\n",
        "\n",
        "        prev_lane_center = lane_center  # Store previous frame value\n",
        "\n",
        "        # Calculate lane offset\n",
        "        frame_center = width // 2\n",
        "        measured_offset = lane_center - frame_center\n",
        "\n",
        "        # Apply Kalman and EMA Filtering\n",
        "        prediction = kalman.predict()\n",
        "        estimated_offset = prediction[0][0]\n",
        "        measurement = np.array([[np.float32(measured_offset)], [0]], np.float32)\n",
        "        kalman.correct(measurement)\n",
        "\n",
        "        global ema_offset\n",
        "        ema_offset = alpha * measured_offset + (1 - alpha) * ema_offset\n",
        "        hybrid_offset = (estimated_offset + ema_offset) / 2\n",
        "\n",
        "        # Overlay lane mask\n",
        "        overlay = frame.copy()\n",
        "        overlay[mask > 0] = [0, 255, 0]  # Green color lane\n",
        "        combined = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
        "\n",
        "        # Display lane offset text\n",
        "        cv2.putText(combined, f\"Hybrid Offset: {int(hybrid_offset)} px\", (10, 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "        out.write(combined)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved as: {output_video}\")\n",
        "\n",
        "# Example usage\n",
        "process_video(\"/content/straight input2.mp4\", \"lane_detection_output.mp4\")\n"
      ],
      "metadata": {
        "id": "3agqNduOgmVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vebJNpjIdEc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}